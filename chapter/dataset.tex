
RENAME THIS CHAPTER DATASET CREATION??????

\section{Dataset creation - should this go inside experiments?}
\label{sec:dataset_creation}
In order to carry out further computational analysis of the topic it is helpful to have an annotated dataset. Since no extensive datasets exist beyond for certain aspectual parameters such as telicity \citep{friedrich-gateva-2017-classification} or stativity \citep{Friedrich2014AutomaticPO}, I had to create my own \citep{friedrich-etal-2023-kind}. The traditional route of hand-annotating including has been a standard paradigm for many years however is highly time-intensive. Crowdsourcing is a more time- and cost-effective way of creating data, however has been shown to be of variable quality \citep{li2024comparative}, especially when the task requires more expert knowledge. Furthermore there are a range of biases which can affect data quality \citep{Beck2023}. The huge success of Large Language Models in recent years has prompted some to look at utilising them for dataset annotation either combined with human annotators \citep{goel2023llms} or on their own \citep{he2023annollm, llmsForPragAndDiscAnalysis, Gilardi_2023}, which is what I opted for.

\subsection{Dataset annotation with LLMs}
Large Language Models' use for annotation data has been demonstrated GIVE SOME EXAMPLES.  

\citet{törnberg2024best} formulated a set of best practices when using LLMs as text annotators, which I aim to be guided by. The paper provides guidelines for those wishing to use LLMs as text annotators, in the absence of labelled data. This situation differs from the "paradigmatic" one described in the paper by the fact that there already exist annotation guidelines and limited labelled data, hence the iterative systematic coding procedure described in the paper is not necessary. Nevertheless \citet{törnberg2024best} provides a well-needed framework for this relatively new approach. 

\subsubsection*{Choice of LM model}
There has been an explosion of LMs in recent years .....

While many previous studies looking at LLM capabilities choose to look at ChatGPT models due to their notoriety in recent years and general good performance. However, as \citet{törnberg2024best} notes, these models come with several issues: it is unknown what the training data for the model is, leading to problems with transparency, and ChatGPT models have been shown to evolve over time, meaning reproducability is hindered. For these reasons, and also in order to host the model locally for better control (rather than using an API), I chose to use a different model. Meta's Llama 2 model seemed to offer a good balance between the apparent current trade-off between performance and scientific good practice, performing comparatively well in previous studies \citep{yuan2023futureTimeLlama} + OTHER EXAMPLES

Due to hardware constraints, I had to use a technique to improve the efficiency of the model training process since the smallest Llama model is 7 billion parameters. In this case I chose to use PEFT (Parameter efficint fine-tuning) (ADD CITATION), which leverages the insight that LM fine-tuning usually only updates parameters at the end of the model. This made it possible to fine-tune the model without resorting to the huge amount of computing power usually necessary for fine-tuning LLMs.

I experimented both with the standard \textsf{Llama-2-7b} and \textsf{Llama-2-7b-chat} \citep{touvron2023llama}, however the latter ended up having difficulties responding in a formulaic way and rather added unnecessary or unrelated information, which is to be expected since it has been fine-tuned to fare well in a dialogue environment. This made it often difficult to extract the model's predicted label to use for evaluation. 

During the course of this study, Llama 3 was released to the public CITE LLAMA 3, promising to have better reasoning skills and be better at following instructions effectively, so I therefore tried and compared both models in comparable settings.

\subsubsection*{Training data}
As the only currently existing data containing UMR aspect classes, I used the example UMR dataset\footnote{ENTER LINK FOR WHERE TO FIND IT!!!!!!} provided by the creators of UMR. It contains HOW MANY sentences in 6 languages (Arapaho, Chinese, English, Kukama, Navajo and Sanapana) annotated according to the UMR schema. The texts come from WHERE 

In order to extract the verbs in the sentences I used Stanford NLP's Stanza POS tagger CITEEE, utilising the alignment given in the training data to find the corresponding node in the UMR graph and its aspect annotation. Table \ref*{umr_aspect_data} shows the class distribution of verbal events extracted from the dataset.
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        Class & No. examples in UMR data & No. examples in upsampling data & \textbf{Total} \\ \hline
        Performance & 124 & 1 & 125\\
        State & 55 & 0 & 55\\
        Activity & 36 & 2 & 38\\
        Endeavour & 17 & 14 & 31\\
        Habitual & 2 & 31 & 33\\ \hline
        \textbf{Total} & 234 & 48 & 282
    \end{tabular}
    \caption{Aspect classes of annotated verbal events in the UMR dataset.}
\end{table}
\label{umr_aspect_data}

Here it is clear that the \emph{habitual} class is underrepresented, corroborating the results of \citet{Dahl1985TenseAA}'s study concluding that habituals and related aspect classes have a low frequency in actual use, and hence I manually added some datapoints to improve the balance. The added sentences were either found in existing online datasets and simly labelled with a UMR aspect class by hand or they were both manually composed and then labelled.

\citet{törnberg2024best} points out that, since it is unknown exactly which training data was used to train many LMs, one should exercise caution when evaluating their performance on a test set, since the model may have seen the test data before during pre-training. In this case, while it is impossible to rule out that the UMR dataset was used for Llama 2 pretraining, it is highly unlikely that it has seen the data in this form (i.e. as a sentence, a verb from this sentence and a UMR aspect label for this verb), and since the model was pre-trained with a next-token prediction task, it is very improbable that it would have memorised the labels for the data it is being tested on in my experiments. Nevertheless this cannot be ruled out and must be kept in mind when analysing the results.

One interesting feature of the UMR \texttt{:aspect} parameter, is that it is used not only with verbs in the source sentence but also with nouns and adjectives (see for example \ref{nominal_event}).

\begin{figure}[t]
    (s1p / publication-91 \\
    \null\quad :ARG1 (s1l / landslide-01 \\
    \null\quad \quad:ARG3 (s1a / and \\
    \null\quad \quad\quad:op1 (s1d / die-01 \\
    \null\quad \quad\quad\quad:ARG1 (s1p3 / person :quant 200) \\
    \null\quad \quad\quad\quad:aspect state) \\
    \null\quad \quad\quad:op2 (s1f / fear-01 \\
    \null\quad \quad\quad\quad:ARG1 (s1m / miss-01 \\
    \null\quad \quad\quad\quad\quad:ARG1 (s1p2 / person :quant 1500) \\
    \null\quad \quad\quad\quad\quad:aspect state) \\
    \null\quad \quad\quad\quad:aspect state) \\
    \null\quad \quad\quad:aspect process) \\
    \null\quad :place (s1c / country :wiki "Philippines"  \\
    \null\quad \quad:name (s1n / name :op1 "Philippines"))))
    \caption{UMR graph of \emph{200 dead, 1,500 feared missing in Philippines landslide.}}
\end{figure}
\label{nominal_event}

Since Russian NLP tools are scarce, it would be difficult to perform event extraction, so, in order to keep the results comparable between both languages, I chose to just to focus on verbal events. In my analysis the \texttt{:aspect} parameter occurred with a verb in the source sentence $74.8\%$ of the time, meaning most of the data from the UMR dataset could be used.

\subsubsection*{Prompt engineering}
Prompt engineering is a new but important field in NLP, and studies have shown the importance of good prompts when dealing with LLMs \citep{kaddour2023challenges, hsieh2023automatic, sahoo2024systematic}. \citet{törnberg2024best} recommends an iterative prompt engineering process of 

The model was given the following instruction for each datapoint:
\begin{quotation}
    The annotation distinguishes five base level aspectual values—State, Habitual, Activity, Endeavor, and Performance. The State value corresponds to stative events: no change occurs during the event. It also includes predicate nominals (be a doctor), predicate locations (be in the forest), and thetic (presentational) possession (have a cat). The Habitual value is annotated on events that occur regularly in the past or present. The Activity value indicates an event has not necessarily ended and may be ongoing at Document Creation Time (DCT). Endeavor is used for processes that end without reaching completion (i.e., termination), whereas Performance is used for processes that reach a completed result state. 
\end{quotation}
followed by the following question:
\begin{quotation}
    Which class does "\emph{\{verb\}}" belong to in this sentence: state, habitual, activity, endeavor, or performance?"
\end{quotation}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}\hline
        \textbf{Prompt type} & \textbf{Llama 2 7B} & \textbf{Llama 3 8B} \\ \hline
        No definitions & 0.198 & 0.479 \\ \hline
        Normal  & 0.748 & 0.769 \\\hline
        Long & 0.688 & 0.740  \\ \hline
    \end{tabular}
    \caption{F1 score on test set from fine-tuning Llama 2 7B and 3 8B on different types of prompt}
\end{table}

The results show that without an explanation of the classes, the model fails, achieving roughly random accuracy. WHICH EPOCH ARE THESE RESULTS FORMATINGG

Interestingly the results also show that the extended prompt (WHERE TO LINK THIS) also exhibited poorer performance. This could be due to the fact that it contained more redundant information which is 

\subsubsection*{Quantitative analysis of results}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}\hline
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
        Performance & 0.90 & 0.56 & 0.69 & 16\\ \hline
        State & 0.00 & 0.00 & 0.00 & 2\\\hline
        Activity & 0.88 & 0.58 & 0.70 & 12\\\hline
        Endeavour & 0.50 & 0.50 & 0.50 & 4\\\hline
        Habitual & 0.76 & 1.00 & 0.86 & 37\\ \hline \hline
        \textbf{Accuracy} &  &  & 0.77 & 71 \\ \hline
        \textbf{Macro Avg.} & 0.61  & 0.53 & 0.55 & 71 \\ \hline
        \textbf{Weighted Avg.} & 0.77  & 0.77 & 0.75 & 71 \\ \hline

    \end{tabular}
    \caption{Results on test set from fine-tuning Llama 2 on UMR aspect classes without upsampling. (UPDATE FORMATINGG!!!!!)}
\end{table}
\label{llama_results_no_upsampling}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}\hline
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
        Performance & 0.78 & 0.88 & 0.82 & 16\\ \hline
        State & 1.00 & 0.75 & 0.86 & 8\\\hline
        Activity & 1.00 & 0.56 & 0.71 & 9\\\hline
        Endeavour & 0.89 & 0.62 & 0.73 & 13\\\hline
        Habitual & 0.81 & 0.97 & 0.88 & 39\\ \hline \hline
        \textbf{Accuracy} &  &  & 0.84 & 85 \\ \hline
        \textbf{Macro Avg.} & 0.90  & 0.75 & 0.80 & 85 \\ \hline
        \textbf{Weighted Avg.} & 0.85  & 0.84 & 0.83 & 85 \\ \hline

    \end{tabular}
    \caption{Results on test set from fine-tuning Llama 2 on UMR aspect classes with upsampling. (UPDATE FORMATINGG!!!!!)}
\end{table}
\label{llama_results_upsampling}

The models seemed to learn the aspect classes relatively well, and it is clear that in general, manual upsampling improved results across the board, as is to be expected. However, the comparison of this result to the one on the model trained without upsampled data should be taken with a grain of salt, since the testing set also included some upsampled data (as there were not enough of some classes - mostly habituals - to make it a fair test), which are by design more prototypical and thus "easier" than the natural data. Nevertheless, this result taken as a standalone one shows that the model was capable of recognising occurrences of the five UMR aspect classes with relatively high accuracy.

\subsection{(Manual!!) Ambiguity annotation}
Since there are no existing datasets for aspectual ambiguity, I decided to annotate the training data I created for the Llama model. This means that I end up with a database of ambiguous sentences which can later be used for testing. This is a relatively hard task for annotation since it is rather open-ended (with $\sum_{i=1}^{5} \binom{5}{i}=31$ different possible annotations for a verb-sentence pair), and the interpretation of aspect is often rather a subjective process.

The annotation was done by an English native-speaker with a background in philosophy and myself. The annotators were given a detailed description of the UMR classes and asked to assign a class to each sentence-verb pair. If several readings were possible, the annotators were asked to write down all possibilities, indicating an aspectually ambiguous sentence. The annotators were also given the possibility to indicate when they were unsure on a particular sentence. See \ref{App:annguide} for the exact annotation guidelines.

For example, the following sentence-verb pair was annotated as belonging to \texttt{State} and \texttt{Activity} classes and hence has an ambiguous aspectual reading:

\begin{exe}
    \ex[] {The first footage from the devastated village showed a sea of mud covering what had been lush green valley farmland. \textbf{covering}}
\end{exe}
\label{ambigExampleSent}

\subsubsection{(Manual!!) Annotation results}

The results of the annotations can be seen in table \ref{table:annot_results}.

\begin{table}
    \begin{tabular}{|c|c|c|c|c|}\hline
        & \multicolumn{2}{|c|}{Annotator 1} & \multicolumn{2}{|c|}{Annotator 2} \\
         & Absolute freq. & Proportion & Absolute freq. & Proportion  \\ \hline
        Agreement with UMR\footnotemark & 202 & 0.706 & 202 & 0.706 \\ \hline
        Several labels (ambiguous) & 71 & 0.248 & 71 & 0.248 \\\hline
        Unsure & 28 & 0.098 & 28 & 0.098\\\hline
    \end{tabular}
    \footnotetext{This is defined as at least one of the classes labelled being the same as the class in the UMR dataset.}
    \caption{Analysis of annotated data. GET FOOTNOTE TO WORK HERE!!! AND UPDATE FOR ANNOTATOR 2}
\end{table}
\label{table:annot_results}

It was interesting to note that the majority of occurrences of the habitual class (30 out of 34 or 88\% AND WHAT FOR ANNOTATOR 2) were also annotated with other labels, empirically motivating the theory that habituality is located in a different level of aspectual distinction than the other classes. This is also reflected in the UMR aspect lattice (see figure \ref{fig:umr_aspect_tree}), where habituals are on the second highest level of the tree. 

Almost all (WHAT NUMBER) the sentence-verb pairs marked with several verb classes by annotator 1 were also marked as ambiguous by annotator 2, however only WHAT PERCENTAGE of those classified as ambiguous by the latter were seen as such by annotator 1. This causes a problem with 

(DIFFERENT SETUP NOW)This presents a problem in how to design the experiment set-up. Since we want to encourage the model to learn prototypical examples of aspect classes to make sure it is not overly confident on ambiguous datapoints, it would make sense to remove the datapoints labelled as having more than one class from training set. However, this would lead to the habitual class being almost non-existant in the dataset. One solution would be to leave them in labelled as habituals, but this would lead to WHAT WOULD IT LEAD TO

I therefore chose to keep datapoints which are 

\subsubsection{Consequences for aspectology}
Habitual on a different level

\subsection{Larger dataset}
In order to fine-tune the smaller model, a larger dataset is needed than the 209 sentences in the UMR example dataset. The only requirements for this larger dataset are that it is representative, clean and has enough exmaples to train the smaller model. It would also be an advantage if it is a sentence-aligned multilingual corpus, since this would make it possible to test the performance of a multilingual model in other languages where there are no labelled data from the Llama model. 

WHY DID WE TAKE OUT HABITUAL? AND NOT ITERATIVE??
