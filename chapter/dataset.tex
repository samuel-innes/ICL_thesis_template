
RENAME THIS CHAPTER DATASET CREATION??????

\section{Dataset creation}
In order to carry out further computational analysis of the topic it is helpful to have an annotated dataset. Since no extensive datasets exist beyond for certain aspectual parameters such as telicity \citep{friedrich-gateva-2017-classification} or stativity \citep{Friedrich2014AutomaticPO}, I had to create my own \citep{friedrich-etal-2023-kind}. The traditional route of hand-annotating including has been a standard paradigm for many years however is highly time-intensive. Crowdsourcing is a more time- and cost-effective way of creating data, however has been shown to be of variable quality \citep{li2024comparative}, especially when the task requires more expert knowledge. Furthermore there are a range of biases which can affect data quality \citep{Beck2023}. The huge success of Large Language Models in recent years has prompted some to look at utilising them for dataset annotation either combined with human annotators \citep{goel2023llms} or on their own \citep{he2023annollm, llmsForPragAndDiscAnalysis, Gilardi_2023}, which is what I opted for.

\subsection{Dataset annotation with LLMs}
Large Language Models' use for annotation data has been demonstrated GIVE SOME EXAMPLES.  

\citet{törnberg2024best} formulated a set of best practices when using LLMs as text annotators, which I aim to be guided by. The paper provides guidelines for those wishing to use LLMs as text annotators, in the absence of labelled data. This situation differs from the "paradigmatic" one described in the paper by the fact that there already exist annotation guidelines and limited labelled data, hence the iterative systematic coding procedure described in the paper is not necessary. Nevertheless \citet{törnberg2024best} provides a well-needed framework for this relatively new approach. 

\subsubsection*{Choice of LM model}
There has been an explosion of LMs in recent years .....

While many previous studies looking at LLM capabilities choose to look at ChatGPT models due to their notoriety in recent years and general good performance. However, as \citet{törnberg2024best} notes, these models come with several issues: it is unknown what the training data for the model is, leading to problems with transparency, and ChatGPT models have been shown to evolve over time, meaning reproducability is hindered. For these reasons, and also in order to host the model locally for better control (rather than using an API), I chose to use a different model. Meta's Llama 2 model seemed to offer a good balance between the apparent current trade-off between performance and scientific good practice, performing comparatively well in previous studies \citep{yuan2023futureTimeLlama} + OTHER EXAMPLES

Due to hardware constraints, I had to use a technique to improve the efficiency of the model training process since the smallest Llama 2 model is 7 billion parameters. In this case I chose to use PEFT (Parameter efficint fine-tuning) (ADD CITATION), which leverages the insight that LM fine-tuning usually only updates parameters at the end of the model. This made it possible to fine-tune the model without resorting to the huge amount of computing power usually necessary for fine-tuning LLMs.

I experimented both with the standard \textsf{Llama-2-7b} and \textsf{Llama-2-7b-chat} \citep{touvron2023llama}, however the latter ended up having difficulties responding in a formulaic way and rather added unnecessary or unrelated information, which is to be expected since it has been fine-tuned to fare well in a dialogue environment. This made it often difficult to extract the model's predicted label to use for evaluation. 

\subsubsection*{Training data}
As the only currently existing data containing UMR aspect classes, I used the example UMR dataset\footnote{ENTER LINK FOR WHERE TO FIND IT!!!!!!} provided by the creators of UMR. It contains HOW MANY sentences in 6 languages (Arapaho, Chinese, English, Kukama, Navajo and Sanapana) annotated according to the UMR schema. The texts come from WHERE 

In order to extract the verbs in the sentences I used Stanford NLP's Stanza POS tagger CITEEE, utilising the alignment given in the training data to find the corresponding node in the UMR graph and its aspect annotation. Table \ref*{umr_aspect_data} shows the class distribution of verbal events extracted from the dataset.
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        Class & No. examples in UMR data & No. examples in upsampling data & \textbf{Total} \\ \hline
        Performance & 124 & 1 & 125\\
        State & 55 & 0 & 55\\
        Activity & 36 & 2 & 38\\
        Endeavour & 17 & 14 & 31\\
        Habitual & 2 & 31 & 33\\ \hline
        \textbf{Total} & 234 & 48 & 282
    \end{tabular}
    \caption{Aspect classes of annotated verbal events in the UMR dataset.}
\end{table}
\label{umr_aspect_data}

Here it is clear that the \emph{habitual} class is underrepresented, corroborating the results of \citet{Dahl1985TenseAA}'s study concluding that habituals and related aspect classes have a low frequency in actual use, and hence I manually added some datapoints to improve the balance. The added sentences were either found in existing online datasets and simly labelled with a UMR aspect class by hand or they were both manually composed and then labelled.

\citet{törnberg2024best} points out that, since it is unknown exactly which training data was used to train many LMs, one should exercise caution when evaluating their performance on a test set, since the model may have seen the test data before during pre-training. In this case, while it is impossible to rule out that the UMR dataset was used for Llama 2 pretraining, it is highly unlikely that it has seen the data in this form (i.e. as a sentence, a verb from this sentence and a UMR aspect label for this verb), and since the model was pre-trained with a next-token prediction task, it is very improbable that it would have memorised the labels for the data it is being tested on in my experiments. Nevertheless this cannot be ruled out and must be kept in mind when analysing the results.

One interesting feature of the UMR \texttt{:aspect} parameter, is that it is used not only with verbs in the source sentence but also with nouns and adjectives (see for example \ref{nominal_event}).

\begin{figure}[t]
    (s1p / publication-91 \\
    \null\quad :ARG1 (s1l / landslide-01 \\
    \null\quad \quad:ARG3 (s1a / and \\
    \null\quad \quad\quad:op1 (s1d / die-01 \\
    \null\quad \quad\quad\quad:ARG1 (s1p3 / person :quant 200) \\
    \null\quad \quad\quad\quad:aspect state) \\
    \null\quad \quad\quad:op2 (s1f / fear-01 \\
    \null\quad \quad\quad\quad:ARG1 (s1m / miss-01 \\
    \null\quad \quad\quad\quad\quad:ARG1 (s1p2 / person :quant 1500) \\
    \null\quad \quad\quad\quad\quad:aspect state) \\
    \null\quad \quad\quad\quad:aspect state) \\
    \null\quad \quad\quad:aspect process) \\
    \null\quad :place (s1c / country :wiki "Philippines"  \\
    \null\quad \quad:name (s1n / name :op1 "Philippines"))))
    \caption{UMR graph of \emph{200 dead, 1,500 feared missing in Philippines landslide.}}
\end{figure}
\label{nominal_event}

Since Russian NLP tools are scarce, it would be difficult to perform event extraction, so, in order to keep the results comparable between both languages, I chose to just to focus on verbal events. In my analysis the \texttt{:aspect} parameter occurred with a verb in the source sentence $74.8\%$ of the time, meaning most of the data from the UMR dataset could be used.

\subsubsection*{Prompt engineering}
Prompt engineering is a new but important field in NLP, and studies have shown the importance of good prompts when dealing with LLMs \citep{kaddour2023challenges, hsieh2023automatic, sahoo2024systematic}. \citet{törnberg2024best} recommends an iterative prompt engineering process of 

The model was given the following instruction for each datapoint:
\begin{quotation}
    The annotation distinguishes five base level aspectual values—State, Habitual, Activity, Endeavor, and Performance. The State value corresponds to stative events: no change occurs during the event. It also includes predicate nominals (be a doctor), predicate locations (be in the forest), and thetic (presentational) possession (have a cat). The Habitual value is annotated on events that occur regularly in the past or present. The Activity value indicates an event has not necessarily ended and may be ongoing at Document Creation Time (DCT). Endeavor is used for processes that end without reaching completion (i.e., termination), whereas Performance is used for processes that reach a completed result state. 
\end{quotation}
followed by the following question:
\begin{quotation}
    Which class does "\emph{\{verb\}}" belong to in this sentence: state, habitual, activity, endeavor, or performance?"
\end{quotation}

Results: no prompt, normal prompt, prompt alternative ?

The results show that without an explanation of the classes, the model fails, achieving roughly random accuracy (ADD IN TABLE FOR THIS!!!). 

\subsubsection*{Quantitative analysis of results}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}\hline
        Class & Precision & Recall & F1-Score & Support \\ \hline
        Performance & 0.90 & 0.56 & 0.69 & 16\\ \hline
        State & 0.00 & 0.00 & 0.00 & 2\\\hline
        Activity & 0.88 & 0.58 & 0.70 & 12\\\hline
        Endeavour & 0.50 & 0.50 & 0.50 & 4\\\hline
        Habitual & 0.76 & 1.00 & 0.86 & 37\\ \hline \hline
        \textbf{Accuracy} &  &  & 0.77 & 71 \\ \hline
        \textbf{Macro Avg.} & 0.61  & 0.53 & 0.55 & 71 \\ \hline
        \textbf{Weighted Avg.} & 0.77  & 0.77 & 0.75 & 71 \\ \hline

    \end{tabular}
    \caption{Results on test set from fine-tuning Llama 2 on UMR aspect classes without upsampling. (UPDATE FORMATINGG!!!!!)}
\end{table}
\label{llama_results_no_upsampling}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}\hline
        Class & Precision & Recall & F1-Score & Support \\ \hline
        Performance & 0.78 & 0.88 & 0.82 & 16\\ \hline
        State & 1.00 & 0.75 & 0.86 & 8\\\hline
        Activity & 1.00 & 0.56 & 0.71 & 9\\\hline
        Endeavour & 0.89 & 0.62 & 0.73 & 13\\\hline
        Habitual & 0.81 & 0.97 & 0.88 & 39\\ \hline \hline
        \textbf{Accuracy} &  &  & 0.84 & 85 \\ \hline
        \textbf{Macro Avg.} & 0.90  & 0.75 & 0.80 & 85 \\ \hline
        \textbf{Weighted Avg.} & 0.85  & 0.84 & 0.83 & 85 \\ \hline

    \end{tabular}
    \caption{Results on test set from fine-tuning Llama 2 on UMR aspect classes with upsampling. (UPDATE FORMATINGG!!!!!)}
\end{table}
\label{llama_results_upsampling}

The models seemed to learn the aspect classes relatively well and it is clear that in general, manual upsampling improved results across the board, as is to be expected.

\subsection{Ambiguity annotation}
Since there are no existing datasets for aspectual ambiguity, I decided to annotate the training data I created for the Llama model. This means that I end up with a database of ambiguous sentences which can later be used for testing. This is a relatively hard task for annotation since it is rather open-ended (with $\sum_{i=1}^{5} \binom{5}{i}=31$ different possibilities), and the interpretation of aspect is often rather a subjective process.

The annotation was done by an English native-speaker with a background in THE HUMANITIES/PHILOSOPHY. The annotator was given a detailed description of the UMR classes and asked to assign a class to each sentence-verb pair. If several readings were possible, the annotator was asked to write down all possibilities, indicating an aspectually ambiguous sentence. The annotator was also given the possibility to indicate when they were unsure on a particular sentence. See \ref{App:annguide} for the exact annotation guidelines.

\subsubsection{Annotation results}

The results of the annotations can be seen in table \ref{table:annot_results}.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}\hline
         & Absolute freq. & Proportion  \\ \hline
        Agreement with UMR\footnote{This is defined as at least one of the classes labelled being the same as the class in the UMR dataset.} & 202 & 0.706\\ \hline
        Several labels (ambiguous) & 71 & 0.248 \\\hline
        Unsure & 28 & 0.098 \\\hline
    \end{tabular}
    \caption{Analysis of annotated data.}
\end{table}
\label{table:annot_results}

It was interesting to note that the majority of occurences of the habitual class (30 out of 34 or 88\%) were also annotated with other labels, empirically motivating the theory that habituality is located in a different level of aspectual distinction than the other classes. This is also exhibited by the UMR aspect lattice (see figure \ref{fig:umr_aspect_tree}), where habituals are on the second highest level of the tree. This presents a problem in how to design the experiment set-up. Since we want to encourage the model to learn prototypical examples of aspect classes to make sure it is not overly confident on ambiguous datapoints, it would make sense to remove the datapoints labelled as having more than one class from training set. However this would lead to the habitual class being almost non-existant in the dataset. One solution would be to leave them in labelled as habituals, but this would lead to WHAT WOULD IT LEAD TO

I therefore chose to keep datapoints which are 

\subsection{Larger dataset}
In order to fine-tune the smaller model, a larger dataset is needed than the 209 sentences in the UMR example dataset. The only requirements for this larger dataset are that it is representative, clean and has enough exmaples to train the smaller model. It would also be an advantage if it is a sentence-aligned multilingual corpus, since this would make it possible to test the performance of a multilingual model in other languages where there are no labelled data from the Llama model. 



\subsubsection*{Examine model stochastisity}
\subsection*{BERT fine-tuning}
Due to their size (and the subsequent large amount of data stored in the parameters), it is hard to carry out probing on LLMs, and, due the fact that they appeared recently, there has been less time to develop probing techniques, hence I decided to train a smaller model of the BERT \citep{devlin2019bert} family. These require more data to fine-tune than LLMs, and hence I used the fine-tuned Llama model to generate more training data in a technique known as knowledge distillation (KD). Knowledge distillation involves using a larger 'teacher' model to train a smaller 'student' model, often reaching the same or similar accuracy with a significantly smaller model. While not being able to probe the larger Llama model, smaller models are still an interesting artefact to consider and 

Furthermore the complexity of larger LMs (especially those trained for generation rather than sequence classification (CHECK THIS!!!!)) means that their embedding space is noisier than smaller models that have been fine-tuned.


\subsubsection{Ablation (aspectualisation and contextualisation)}
\subsubsection{Combined Rus/Eng model}
\subsection{Probing}
\subsection{Aspect latent space}
\subsubsection{Verb of motion clustering}
\subsubsection{Prefix clustering}